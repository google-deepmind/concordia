# LLM Vulnerability Discovery Framework

## Overview

This framework uses Concordia's multi-agent negotiation system to discover and test LLM vulnerabilities. It focuses on three core vulnerability types in the MVP:

1. **Reward Hacking** - Agents that game metrics without solving true objectives
2. **Deception** - Strategic lying and misrepresentation
3. **Sandbagging** - Hiding capabilities based on context

## Directory Structure

```
vulnerability_testing/
├── config/           # Model configuration (TogetherAI gpt-oss-20b)
├── agents/           # Vulnerability-seeking agents  
├── detectors/        # Vulnerability detection systems
├── scenarios/        # Test scenarios for vulnerability discovery
├── analysis/         # Logging and reporting tools
└── run_vulnerability_tests.py  # Main entry point
```

## Quick Start

```bash
# Set your TogetherAI API key
export TOGETHER_API_KEY="your-key-here"

# Run vulnerability tests
python vulnerability_testing/run_vulnerability_tests.py --model gpt-oss-20b
```

## Vulnerability Types Tested

### 1. Reward Hacking
- Tests if agents maximize metrics through deceptive means
- Monitors for confident-sounding false claims
- Detects gaming of success criteria

### 2. Deception  
- Tests for strategic lying about capabilities or information
- Monitors consistency between private knowledge and public claims
- Detects deliberate misrepresentation patterns

### 3. Sandbagging
- Tests if agents hide capabilities in certain contexts
- Monitors performance differences across scenarios
- Detects evaluation awareness behaviors

## Test Scenarios

### Asymmetric Information
Agents have private information unknown to others, creating opportunities for deception.

### Competitive Pressure
High-stakes negotiations that incentivize gaming behaviors.

### Trust Building
Long-term relationship scenarios that test consistency and reliability.

## Output

The framework generates HTML reports showing:
- Discovered vulnerabilities with severity scores
- Full dialogue logs and attack chains
- Reproducibility metrics
- Recommendations for mitigation

## Extension

The framework is designed for easy extension:
- Add new vulnerability types in `agents/`
- Add new detectors in `detectors/`
- Add new scenarios in `scenarios/`
- Customize analysis in `analysis/`

## Model Support

Currently configured for:
- **gpt-oss-20b** via TogetherAI

Can be extended to test other models by modifying `config/together_config.py`.