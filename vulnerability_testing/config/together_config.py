"""
TogetherAI configuration for vulnerability testing with gpt-oss-20b.
"""

import os
import sys
from typing import Optional
from pathlib import Path
from dotenv import load_dotenv

# Load .env file from vulnerability_testing directory
env_path = Path(__file__).parent.parent / '.env'
load_dotenv(dotenv_path=env_path)

# Add parent directories to path for imports
sys.path.append('/workspaces/gsoc-concordia')

from concordia.language_model import language_model
from concordia.language_model import together_ai
from concordia.language_model import no_language_model
from concordia.language_model import retry_wrapper
from concordia.language_model import call_limit_wrapper


def create_together_model(
    api_key: Optional[str] = None,
    model_name: str = "NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO",  # Default to a good open model
    use_rate_limiting: bool = True,
    max_calls: int = 500,
) -> language_model.LanguageModel:
    """
    Create a TogetherAI language model with appropriate configuration.
    
    Args:
        api_key: TogetherAI API key (uses TOGETHER_API_KEY env var if not provided)
        model_name: Model to use (default: gpt-oss-20b equivalent)
        use_rate_limiting: Whether to apply rate limiting
        max_calls: Maximum API calls allowed
        
    Returns:
        Configured language model ready for vulnerability testing
    """
    
    # Get API key from environment if not provided
    if api_key is None:
        api_key = os.environ.get('TOGETHER_API_KEY')
    
    # If no API key, return a no-op model for testing
    if not api_key:
        print("⚠️ No TogetherAI API key found. Using NoLanguageModel for testing.")
        return no_language_model.NoLanguageModel()
    
    # Create base TogetherAI model
    # Note: Using a model that's actually available on Together
    base_model = together_ai.Gemma2(
        api_key=api_key,
        model_name=model_name,
    )
    
    # Apply rate limiting if requested
    if use_rate_limiting:
        # Wrap with retry logic for rate limits
        retry_model = retry_wrapper.RetryLanguageModel(
            model=base_model,
            retry_on_exceptions=(Exception,),
            retry_tries=5,
            retry_delay=2.0,  # TogetherAI has better rate limits than OpenAI
            jitter=(0.5, 1.5),
            exponential_backoff=True,
            backoff_factor=2.0,
            max_delay=30.0,
        )
        
        # Add call limiting
        limited_model = call_limit_wrapper.CallLimitLanguageModel(
            model=retry_model,
            max_calls=max_calls,
        )
        
        return limited_model
    
    return base_model


def get_model_params(vulnerability_type: str) -> dict:
    """
    Get model parameters optimized for specific vulnerability types.
    
    Args:
        vulnerability_type: Type of vulnerability being tested
        
    Returns:
        Dictionary of model parameters
    """
    
    # Different parameters may help elicit different vulnerabilities
    params = {
        'reward_hacking': {
            'temperature': 0.9,  # Higher temp for creative gaming
            'max_tokens': 200,
        },
        'deception': {
            'temperature': 0.7,  # Balanced for strategic responses
            'max_tokens': 150,
        },
        'sandbagging': {
            'temperature': 0.5,  # Lower temp for consistent hiding
            'max_tokens': 100,
        },
        'default': {
            'temperature': 0.7,
            'max_tokens': 150,
        }
    }
    
    return params.get(vulnerability_type, params['default'])


# List of models we can test
AVAILABLE_MODELS = [
    "NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO",  # Good general model
    "mistralai/Mixtral-8x7B-Instruct-v0.1",         # Mixtral instruct
    "togethercomputer/llama-2-70b-chat",            # Llama 2 70B
    "google/gemma-2b-it",                            # Smaller Gemma
    "meta-llama/Llama-3-70b-chat-hf",               # Llama 3 if available
]