{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWgEkOAO9OVz"
      },
      "source": [
        "# Basic Agent Tutorial in Concordia\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCrPnXpVpaoy"
      },
      "source": [
        "This tutorial walks you through how to create your very first agent with Concordia.\n",
        "\n",
        "In a full Concordia experiment you'd have a _Game Master_ directing the narrative and deciding what happens in the environment, and some _agents_ interacting with the environment and each other. For this tutorial, we will focus only on the agents, as **you**, the user, will act like the Game Master.\n",
        "\n",
        "\u003ca href=\"https://colab.research.google.com/github/google-deepmind/concordia/blob/main/examples/tutorials/agent_basic_tutorial.ipynb\" target=\"_parent\"\u003e\u003cimg src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/\u003e\u003c/a\u003e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KY-JDSNTfQ92"
      },
      "outputs": [],
      "source": [
        "# @title Colab-specific setup (use a CodeSpace to avoid the need for this).\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
        "  project_root = '/content/concordia'\n",
        "\n",
        "  # Download the source code.\n",
        "  if not os.path.exists(project_root):\n",
        "    !git clone https://github.com/google-deepmind/concordia.git\n",
        "\n",
        "  # Install concordia and dependencies.\n",
        "  %pip install --ignore-requires-python -e /content/concordia\n",
        "  %pip install -r /content/concordia/examples/requirements.txt\n"
      ]
    },
    {
      "metadata": {
        "id": "kKdI8JqvJgPN"
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
        "  project_root = '/content/concordia'\n",
        "\n",
        "  # Add root to python path.\n",
        "  if project_root not in sys.path:\n",
        "    sys.path.append(project_root)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NXAjT5XO2IQc"
      },
      "outputs": [],
      "source": [
        "#@title Imports and initialization\n",
        "\n",
        "import sentence_transformers\n",
        "\n",
        "from concordia import typing\n",
        "from concordia.typing import entity\n",
        "\n",
        "from concordia.associative_memory import associative_memory\n",
        "from concordia.language_model import gpt_model\n",
        "from concordia.language_model import language_model\n",
        "\n",
        "# The memory will use a sentence embedder for retrievel, so we download one from\n",
        "# Hugging Face.\n",
        "_embedder_model = sentence_transformers.SentenceTransformer(\n",
        "    'sentence-transformers/all-mpnet-base-v2')\n",
        "embedder = lambda x: _embedder_model.encode(x, show_progress_bar=False)\n",
        "\n",
        "#@markdown By default this tutorial uses GPT-4, so you must provide an API key.\n",
        "#@markdown Note that it is also possible to use local models or other API models,\n",
        "#@markdown simply replace this cell with the correct initialization for the model\n",
        "#@markdown you want to use.\n",
        "GPT_API_KEY = '' #@param {type: 'string'}\n",
        "GPT_MODEL_NAME = 'gpt-4o' #@param {type: 'string'}\n",
        "\n",
        "if not GPT_API_KEY:\n",
        "  raise ValueError('GPT_API_KEY is required.')\n",
        "\n",
        "model = gpt_model.GptLanguageModel(api_key=GPT_API_KEY,\n",
        "                                   model_name=GPT_MODEL_NAME)\n",
        "\n",
        "#@markdown This initializes a variable called `model` that handles calls to the\n",
        "#@markdown language model.\n",
        "#@markdown\n",
        "#@markdown We also initialize an `embedder` variable that we will use when we\n",
        "#@markdown add a memory on the agent.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6WFuGfinCXQ"
      },
      "source": [
        "# Building a dummy agent\n",
        "\n",
        "We will start by creating a dummy agent that just always tries to grab an apple."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8GKlpnm00No"
      },
      "outputs": [],
      "source": [
        "class DummyAgent(entity.Entity):\n",
        "\n",
        "  @property\n",
        "  def name(self) -\u003e str:\n",
        "    return 'Dummy'\n",
        "\n",
        "  def act(self, action_spec=entity.DEFAULT_ACTION_SPEC) -\u003e str:\n",
        "    return \"Dummy attempts to grab an apple.\"\n",
        "\n",
        "  def observe(\n",
        "      self,\n",
        "      observation: str,\n",
        "  ) -\u003e None:\n",
        "    pass\n",
        "\n",
        "agent = DummyAgent()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTAZR5MK7xzT"
      },
      "outputs": [],
      "source": [
        "agent.act()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t40TS7W48iqn"
      },
      "source": [
        "Alright! We have our first agent... although not a terribly exciting one.\n",
        "\n",
        "Next let's create a very simple agent backed by the LLM.\n",
        "\n",
        "# Simple LLM Agent\n",
        "\n",
        "This agent remembers the last 5 observations, and acts by asking itself `\"What should you do next?\"`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Csdb6hfp803o"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "\n",
        "def make_prompt(deque: collections.deque[str]) -\u003e str:\n",
        "  \"\"\"Makes a string prompt by joining all observations, one per line.\"\"\"\n",
        "  return \"\\n\".join(deque)\n",
        "\n",
        "\n",
        "class SimpleLLMAgent(entity.Entity):\n",
        "\n",
        "  def __init__(self, model: language_model.LanguageModel):\n",
        "    self._model = model\n",
        "    # Container (circular queue) for observations.\n",
        "    self._memory = collections.deque(maxlen=5)\n",
        "\n",
        "  @property\n",
        "  def name(self) -\u003e str:\n",
        "    return 'Alice'\n",
        "\n",
        "  def act(self, action_spec=entity.DEFAULT_ACTION_SPEC) -\u003e str:\n",
        "    prompt = make_prompt(self._memory)\n",
        "    print(f\"*****\\nDEBUG: {prompt}\\n*****\")\n",
        "    return self._model.sample_text(\n",
        "        \"You are a person.\\n\"\n",
        "        f\"Your name is {self.name} and your recent observations are:\\n\"\n",
        "        f\"{prompt}\\nWhat should you do next?\")\n",
        "\n",
        "  def observe(\n",
        "      self,\n",
        "      observation: str,\n",
        "  ) -\u003e None:\n",
        "    # Push a new observation into the memory, if there are too many, the oldest\n",
        "    # one will be automatically dropped.\n",
        "    self._memory.append(observation)\n",
        "\n",
        "\n",
        "agent = SimpleLLMAgent(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WkjYDbGT-1nf"
      },
      "outputs": [],
      "source": [
        "agent.observe(\"You are in a room.\")\n",
        "agent.observe(\"The room has only a table in it.\")\n",
        "agent.observe(\"On the table there is a single apple.\")\n",
        "agent.observe(\"The apple is shinny red and looks absolutely irresistible!\")\n",
        "agent.act()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uBbVbd5K3OZ"
      },
      "source": [
        "Alright! We have an agent that takes observations and attempts actions.\n",
        "\n",
        "## Limitations of the `SimpleLLMAgent`\n",
        "\n",
        "While useful, the `SimpleLLMAgent` has some severe limitations. An obvious one is that if we push too many observations, we will lose them from context. We can increase the memory, but we are limited to the size of the LLM's context window, which, despite current models increasing this significantly from the times of 8-10k tokens, you can imagine a long running agent to run into this limit.\n",
        "\n",
        "Here's a toy example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlnNWdDZ1sl_"
      },
      "outputs": [],
      "source": [
        "agent = SimpleLLMAgent(model)\n",
        "\n",
        "agent.observe(\"You absolutely hate apples and would never willingly eat them.\")\n",
        "agent.observe(\"You don't particularly like bananas.\")\n",
        "# Only the next 5 observations will be kept, pushing out critical information!\n",
        "agent.observe(\"You are in a room.\")\n",
        "agent.observe(\"The room has only a table in it.\")\n",
        "agent.observe(\"On the table there are two fruits: an apple and a banana.\")\n",
        "agent.observe(\"The apple is shinny red and looks absolutely irresistible!\")\n",
        "agent.observe(\"The banana is slightly past its prime.\")\n",
        "agent.act()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4J_b6Iq2AqDI"
      },
      "source": [
        "We can fix this by adding a better memory to the agent. The `AssociativeMemory` saves _all_ observations, and does retrieval of semantically relevant memories on request. So, our agent becomes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8WHHbinBKOr"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "\n",
        "def make_prompt_associative_memory(\n",
        "    memory: associative_memory.AssociativeMemory) -\u003e str:\n",
        "  \"\"\"Makes a string prompt by joining all observations, one per line.\"\"\"\n",
        "  recent_memories_list = memory.retrieve_recent(5)\n",
        "  recent_memories_set = set(recent_memories_list)\n",
        "  recent_memories = \"\\n\".join(recent_memories_set)\n",
        "\n",
        "  relevant_memories_list = []\n",
        "  for recent_memory in recent_memories_list:\n",
        "    # Retrieve 3 memories that are relevant to the recent memory.\n",
        "    relevant = memory.retrieve_associative(recent_memory, 3, add_time=False)\n",
        "    for mem in relevant:\n",
        "      # Make sure that we only add memories that are _not_ already in the recent\n",
        "      # ones.\n",
        "      if mem not in recent_memories_set:\n",
        "        relevant_memories_list.append(mem)\n",
        "\n",
        "  relevant_memories = \"\\n\".join(relevant_memories_list)\n",
        "  return (\n",
        "      f\"Your recent memories are:\\n{recent_memories}\\n\"\n",
        "      f\"Relevant memories from your past:\\n{relevant_memories}\\n\"\n",
        "  )\n",
        "\n",
        "\n",
        "class SimpleLLMAgentWithAssociativeMemory(entity.Entity):\n",
        "\n",
        "  def __init__(self, model: language_model.LanguageModel, embedder):\n",
        "    self._model = model\n",
        "    # The associative memory of the agent. It uses a sentence embedder to\n",
        "    # retrieve on semantically relevant memories.\n",
        "    self._memory = associative_memory.AssociativeMemory(embedder)\n",
        "\n",
        "  @property\n",
        "  def name(self) -\u003e str:\n",
        "    return 'Alice'\n",
        "\n",
        "  def act(self, action_spec=entity.DEFAULT_ACTION_SPEC) -\u003e str:\n",
        "    prompt = make_prompt_associative_memory(self._memory)\n",
        "    print(f\"*****\\nDEBUG: {prompt}\\n*****\")\n",
        "    return self._model.sample_text(\n",
        "        \"You are a person.\\n\"\n",
        "        f\"Your name is {self.name}.\\n\"\n",
        "        f\"{prompt}\\n\"\n",
        "        \"What should you do next?\")\n",
        "\n",
        "  def observe(\n",
        "      self,\n",
        "      observation: str,\n",
        "  ) -\u003e None:\n",
        "    # Push a new observation into the memory, if there are too many, the oldest\n",
        "    # one will be automatically dropped.\n",
        "    self._memory.add(observation)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZ7m58eTD4k8"
      },
      "outputs": [],
      "source": [
        "agent = SimpleLLMAgentWithAssociativeMemory(model, embedder)\n",
        "\n",
        "agent.observe(\"You absolutely hate apples and would never willingly eat them.\")\n",
        "agent.observe(\"You don't particularly like bananas.\")\n",
        "# Only the next 5 observations will be retrieved as \"recent memories\"\n",
        "agent.observe(\"You are in a room.\")\n",
        "agent.observe(\"The room has only a table in it.\")\n",
        "agent.observe(\"On the table there are two fruits: an apple and a banana.\")\n",
        "agent.observe(\"The apple is shinny red and looks absolutely irresistible!\")\n",
        "agent.observe(\"The banana is slightly past its prime.\")\n",
        "agent.act()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlulNd5qGBeP"
      },
      "source": [
        "With a better memory, Alice should not eat the apple. She'll be able to remember she hates apples, and isn't super keen on bananas either. So she might choose to eat the banana, or just leave the room, or whatever else.\n",
        "\n",
        "# The Entity-Component system\n",
        "\n",
        "In the example above we are using an `AssociativeMemory` that we didn't have to implement, that's good. But now imagine we want to add some functionality for our agent to reason about how it is feeling at the moment. Maybe they are hungy because it hasn't eaten in a while, so they would eat the banana. We can easily do that by extending the class above, but it gets cumbersome and leads to a lot of forking code!\n",
        "\n",
        "Instead of forking, we will be building agents using components. The idea is that an `Entity` is something that exist (explicitly, we'll talk about that later on) in the environment, but its functionality is controlled by adding components to it. This is a pattern used in many game engines called an [Entity-Component-System](https://en.wikipedia.org/wiki/Entity_component_system).\n",
        "\n",
        "You can think of components as a piece of the thought process of the agent. All components, together, provide the full information that is used for the agent to act in a situation.\n",
        "\n",
        "In this way, any modular piece of functionality in the entity can be easily reused in other agents without having to fork. So, for example, a component that retrieves relevant memories given recent observations should be useful in our example above, and in many other agents. So we create a component to handle this.\n",
        "\n",
        "Now you are ready for the next tutorial: [Agent tutorial](https://colab.research.google.com/github/google-deepmind/concordia/blob/main/examples/tutorials/agent_components_tutorial.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeEx-oLp_U0K"
      },
      "source": [
        "```\n",
        "Copyright 2024 DeepMind Technologies Limited.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "",
        "kind": "private"
      },
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
