# Language Model Wrappers: A Call for Community Contribution

This document outlines the process for adding and maintaining language model
wrappers in Concordia. As the landscape of language model APIs is constantly
evolving, we rely on the community to help us keep these wrappers up-to-date and
to expand our support for a wider variety of models. Your contributions are
crucial for the continued success and utility of this project.

---

## Why We Need Your Help

The `language_model` wrappers in Concordia provide a standardized interface for
interacting with various language models. However, the underlying APIs for these
models can change frequently, leading to breakages in our existing wrappers. To
ensure that Concordia remains a robust and reliable tool for the community, we
need your help in maintaining these wrappers and adding new ones.

By contributing to this effort, you can:

* **Ensure continued access** to your preferred language models within
Concordia.
* **Expand the capabilities** of Concordia by adding support for new and
emerging models.
* **Become an active member** of the Concordia community and collaborate with
other researchers and developers.

---

## How to Contribute

We welcome and encourage pull requests that add new language model wrappers or
fix existing ones. Hereâ€™s how you can get started:

### Adding a New Language Model Wrapper

If you want to add a wrapper for a new language model, please follow these
steps:

1.  **Create a new file** in the `concordia/language_model/` directory. You can
use the existing wrappers as a template.
2.  **Implement the two main functions**:
    * `sample_text(prompt)`: This function should take a prompt string and
    return a text completion from the language model.
    * `sample_choice(prompt, choices)`: This function should take a prompt and a
    list of choices, and return one of the choices.
3.  **Add a new case** to the `switch` statement in the `setup` utility located
at `concordia/language_model/utils.py` to integrate your new wrapper.
4.  **Submit a pull request** with your changes. We will review it and merge it
as soon as possible.

When implementing `sample_choice`, the approach often depends on whether you
have access to the model's internal states (i.e., weights and tokenwise
predicted log probabilities):

*   **Open-Weights Approach**: For models where you can access the tokenwise
    predicted log probabilities, the best method is to append the answer choices
    to the prompt and compare the log probabilities produced by the model
    generating each choice. This provides a more direct way of measuring the
    model's preference. The `DefaultCompletion` wrapper in `together_ai.py` is a
    good example of this approach.

*   **Closed-Weights Approach**: For models accessed via APIs that don't expose
    tokenwise predicted log probabilities (like many commercial models), the
    common approach is to ask the model in the prompt to return a response that
    exactly matches one of the provided choices. This requires careful parsing
    and error handling for cases where the model's output doesn't perfectly
    match one of the options. The `BaseGPTModel` wrapper in `base_gpt_model.py`
    demonstrates this approach.

### Maintaining Existing Wrappers

If you notice that an existing wrapper is broken due to an API change, we
encourage you to fix it. You can do this by:

1.  **Identifying the source of the breakage** in the wrapper's code.
2.  **Updating the code** to be compatible with the new API.
3.  **Submitting a pull request** with your fixes.

Your contributions will help to ensure that Concordia remains a valuable
resource for the entire community. Thank you for your support!
